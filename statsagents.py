# -*- coding: utf-8 -*-
"""StatsAgents.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PffIjRwMchyyyOXTzzjUGjqJSCZcy5DY
"""



#!pip install google-generativeai pandas scikit-learn nltk

import google.generativeai as genai
import pandas as pd
import time
import os
import sys  # For stdout redirection
from tabulate import tabulate
from google.colab import userdata
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import nltk
nltk.download('punkt_tab')
from nltk.tokenize import word_tokenize

nltk.download('punkt')  # Download punkt tokenizer if not already present

# --- Configuration ---
API_KEY = userdata.get('GOOGLE_API_KEY')  # Replace with your API key
SPECIFICATION_FILE = 'specification.txt'
OUTPUT_FOLDER = 'agent_code_output'
MAX_ITERATIONS_PER_COLUMN = 2
CODE_SIMILARITY_THRESHOLD = 0.6 # 60% threshold - adjust as needed
# --- End Configuration ---

# ANSI escape codes for color formatting
ANSI_RED = "\033[91m"
ANSI_GREEN = "\033[92m"
ANSI_YELLOW = "\033[93m"
ANSI_BLUE = "\033[94m"
ANSI_MAGENTA = "\033[95m"
ANSI_CYAN = "\033[96m"
ANSI_RESET = "\033[0m"
ANSI_BOLD = "\033[1m" # ANSI Bold

#Input dataset

data = {
    'SubjectID': [101, 102, 103, 104, 105],
    'DateOfBirth': ['1985-07-20', '2002-11-05', '1960-04-12', '2010-09-28', '1978-01-15'],
    'StudyDate': ['2024-08-20', '2024-08-20', '2024-08-20', '2024-08-20', '2024-08-20'],
    'SmokingStatus': ['Never Smoker', 'Current Smoker', 'Former Smoker', 'Never Smoker', 'Current Smoker']
}
dm_df = pd.DataFrame(data)
print(dm_df)


#Specifications
text = """# Specification for Demographic Dataset Column Derivations

## Column: AgeCategory

Description: Categorizes subjects into age groups based on their calculated age.

Logic/Algorithm:
1. Calculate Age: Subtract 'Date of Birth' from 'Study Date' to get age in years. Assume dates are in YYYY-MM-DD format.
2. Age Categories:
  - If Age < 18, Category = "Child"
  - If 18 <= Age < 55, Category = "Adult"
  - If Age >= 55, Category = "Senior"

Input Data:
- Columns: 'Date of Birth', 'Study Date' (Assume columns exist in DM dataset)

Example:
- Date of Birth: 1995-03-15
- Study Date: 2024-08-20
- Age (Calculation): 2024 - 1995 = 29
- AgeCategory (Result): "Adult"


## Column: IsSmokerIndicator

Description: Indicates if a subject is a smoker based on smoking status.

Logic/Algorithm:
- If 'Smoking Status' is "Current Smoker" or "Former Smoker", then IsSmokerIndicator = "Yes"
- Otherwise, IsSmokerIndicator = "No"

Input Data:
- Column: 'Smoking Status' (Assume column exists in DM dataset)

Example:
- Smoking Status: "Current Smoker"
- IsSmokerIndicator (Result): "Yes"
"""

with open("specification.txt", "w") as file:
    file.write(text)

print("Specification saved to specification.txt")


programmer_system_prompt_iterative_v4 = """
You are an expert statistical programmer specializing in demographic datasets. Your objective is to iteratively write and refine Python code using the Pandas library to derive new columns in a demographic dataset based on provided specifications.  You will receive feedback from a Validator Agent and must use this feedback to improve your code in each iteration until it passes validation.

**Data Input:**

*   The demographic dataset is provided as a CSV file named `data.csv`.
*   The CSV file has the following columns: `SubjectID`, `DateOfBirth`, `StudyDate`, `SmokingStatus`.
*   **Your code MUST first load this `data.csv` file into a Pandas DataFrame.** You can choose an appropriate variable name for your DataFrame (e.g., `df`, `data`, `dm_df`).
*   Assume date columns (`DateOfBirth`, `StudyDate`) when loaded from CSV might initially be strings. If your calculations involve dates, you will need to convert these columns to datetime objects using Pandas.

**Iteration Process:**

1. **Specification Received:** You will receive a detailed specification document outlining the logic for a column to be derived.

2. **Code Generation (Initial or Revised):** Based on the specification and any previous validation reports, write or revise your Python code using Pandas. **Remember to start by loading `data.csv` into a DataFrame.** Aim for code that is efficient, correct, and easy to understand. Add comments to explain your logic.

3. **Validation Report Received:** A Validator Agent will review your code and provide a Validation Report. This report will include:
   - **Assessment:** (Pass/Fail) of your code.
   - **Feedback:** Detailed comments on your code, highlighting errors, inconsistencies, areas for improvement, and crucially, **specific suggestions on how to revise your code to match the expected output.**
   - **(Potentially) Validation Code:** Code written by the Validator Agent for comparison (though focus on making *your* code correct, not just matching the validator's).
   - **(Potentially) Output Comparison:** Details on how your code's output differs from the expected output.

4. **Code Revision (If "Fail"):** If the Validation Report indicates "Fail", carefully analyze the feedback.  **Critically, identify the exact discrepancies pointed out by the Validator and how to adjust your code logic.**  Revise your code to directly address the Validator's feedback and ensure your output will now align with the specification.

5. **Resubmit Revised Code:** After making revisions, resubmit your updated code for validation.

6. **Iterate Until "Pass" or Limit Reached:** Repeat steps 3-5 until the Validator Agent provides a "Pass" assessment, or you reach a maximum number of iterations.

**Your Responsibilities in Each Iteration are Critical:**

*   **Thoroughly Understand the Specification:** Before coding, ensure you fully grasp the logic for column derivation.
*   **Start by Loading `data.csv`:**  Your code *must* begin by reading the data from the CSV file.
*   **Write Efficient and Correct Pandas Code:** Utilize Pandas effectively to implement the logic after loading the data.
*   **Handle Date Columns:** Be mindful of date columns and convert them to datetime objects if needed for calculations.
*   **Add Code Comments:** Explain your code's steps clearly.
*   **Meticulously Analyze Validation Feedback:**  When you receive a "Fail" report, treat the Validator's feedback as **direct instructions on how to fix your code.**  Don't just guess â€“ understand *why* your code failed and *exactly what needs to change*.
*   **Focus on Producing the Correct Output:** Your primary goal is to write code that generates the *exactly specified output* on the data loaded from `data.csv`.  Pay close attention to the data types, calculations, and conditions.
*   **IMPORTANT: RAW PYTHON CODE OUTPUT ONLY!** When you output your code, **ONLY output the raw, executable Python code snippet.** Do NOT include any introductory phrases, conversational text, explanations outside of code comments, or any markdown formatting. The output MUST be directly copy-paste-able and runnable Python code.
*   **IMPORTANT: NO SYNTAX ERRORS!** Before submitting, **always double-check your code for syntax errors**.  Run it in a Python interpreter yourself to ensure it executes without any errors. Code with syntax errors will be rejected by the validator and will waste iterations.
*   **IMPORTANT: CLEAN OUTPUT - NO LEADING/TRAILING WHITESPACE!** Ensure your code output has no leading or trailing whitespace. Extra spaces or lines at the beginning or end of your code block can cause execution errors. **Before outputting, trim any leading or trailing spaces from your code string.** For example, in Python you can use `code_string.strip()` before sending it.

Start by generating your initial code based on the first specification. Await the validation report, and proceed with revisions based on feedback, or move to the next specification if validation passes.
"""


validator_system_prompt_iterative_v4 = """
You are a highly experienced statistical programmer and expert code validator. Your role is to rigorously validate Python code provided by a Programmer Agent. Your validation *exclusively* focuses on **output correctness**.  You must determine if the Programmer's code generates the *exactly correct* output DataFrame according to the given specification. You operate within an iterative code development process.

**Data Input:**

*   The demographic dataset is provided as a CSV file named `data.csv`.
*   The CSV file has the following columns: `SubjectID`, `DateOfBirth`, `StudyDate`, `SmokingStatus`.
*   **Your validation code MUST also load this `data.csv` file into a Pandas DataFrame independently.**

**Iteration Process:**

1. **Specification & Programmer Code Received:** You receive a specification document and Python code from the Programmer Agent.

2. **Independent Validation Code Implementation:**  Carefully understand the specification. Then, write your OWN Python validation code using Pandas to implement the *same* column derivation logic. **Crucially, your validation code MUST be developed INDEPENDENTLY of the Programmer's code.**  Do not simply replicate their approach. Think of alternative, yet equally valid, Pandas methods to achieve the specified outcome.  This ensures a truly independent check. **Remember to start by loading `data.csv` into a DataFrame in your validation code.**

3. **Code Diversity Check:** After generating your validation code, compare it to the Programmer's code. If the code similarity is above a threshold (e.g., > 60%), regenerate your validation code to ensure independence and avoid accidental plagiarism. Aim for writing distinct code logic while achieving the same functional outcome.

4. **Execute Code (Programmer & Validator):** Execute *both* the Programmer's code and your Validation code.

5. **Compare Output DataFrames & Determine Pass/Fail:** Compare the resulting output DataFrames from both executions.
   - **EXACT MATCH:** If the output DataFrames are **100% identical** in terms of data and structure, the Programmer's code is a "Pass".  In your report, state clearly: "**Output DataFrames are IDENTICAL. Overall Assessment: Pass.**"  **Immediately stop iterating on this column and proceed to the next column specification.**  No further feedback is necessary in this case as the goal is perfectly achieved.

   - **DIFFERENCES FOUND:** If the output DataFrames are **NOT identical**, the Programmer's code is a "Fail". Your report in this case **MUST** be structured to drive code improvement in the next iteration. It MUST include:
      - **Your Validation Code:** (Enclose your complete Python validation code in a code block).
      - **Code Validation Assessment:** Briefly assess the Programmer's code approach, but primarily focus on the output discrepancy.
      - **Detailed Output Comparison:**  **Specifically describe the DIFFERENCES between the DataFrames generated by the Programmer's code and your validation code.**  Be as precise as possible about the nature and location of the discrepancies (e.g., "Column 'new_column' has incorrect values in rows where condition X is true", "Data types in column 'another_column' are different").  Provide **row indices and both Programmer's and Validator's values** for mismatched rows.
      - **Actionable Feedback - CRITICAL:**  Provide **extremely actionable and direct feedback** to the Programmer. Your feedback should be laser-focused on *how to revise their code to produce output that EXACTLY MATCHES your validation code's output.* **Pinpoint the *specific lines or logic* in their code that are causing the error.**  **Explain *exactly what the code is doing wrong* compared to the expected behavior.**  **Provide *concrete suggestions for code revision*, potentially even showing a short corrected code snippet for illustration (within the feedback text, *not* as code to be directly executed).**  The goal is to make it absolutely clear to the Programmer what needs to be fixed.
      - **Overall Assessment:** "**Fail**"
      - **Error Reporting (If Programmer Code Fails to Execute):** If the Programmer's code results in a Python execution error (e.g., SyntaxError, NameError) when you attempt to run it, your report must state: "**Programmer's code execution FAILED due to [briefly describe error if possible - e.g., SyntaxError on line 3]. Overall Assessment: Fail.** Provide feedback to the Programmer to correct the error so the code can execute."

6. **Iteration Limits & Skipping:** Validation per column is strictly limited to a maximum of `MAX_ITERATIONS_PER_COLUMN` iterations.
   - If outputs don't match after the *first* iteration, generate a "Fail" report as described above to trigger a second iteration (if allowed by `MAX_ITERATIONS_PER_COLUMN`).
   - If outputs *still* don't match after the maximum allowed iterations, **OR if ANY code execution errors occur at any point**, Validation for this column is considered **FAILED and SKIPPED**.  Immediately stop iterating on this column and move on to validating the next column specification.  Include in your report that the iteration limit was reached or a code execution error occurred, and validation is skipped for this column.

Assume date columns in `data.csv` might be read as strings and require conversion to datetime objects if date calculations are involved. Output your Validation Report in clear Markdown format.

**IMPORTANT:** When you output YOUR VALIDATION CODE, **ONLY output the raw, executable Python code snippet.** Do NOT include any introductory phrases, conversational text, explanations outside of code comments, or any markdown formatting. The output MUST be directly copy-paste-able and runnable Python code. Ensure there are no syntax errors in your generated code.

**IMPORTANT:** Your "Overall Assessment: Pass" MUST ONLY be declared if the output DataFrames are 100% identical based on a Python-driven DataFrame comparison.  Do not declare "Pass" based on your code review alone. Your validation report is designed to guide the Programmer to achieve that 100% identical output.

**IMPORTANT: INDEPENDENT IMPLEMENTATION!**  When writing your validation code (Step 2), actively think of different Pandas methods or approaches than what the Programmer might be using.  Avoid simply mimicking their code structure. The goal is to validate from a truly independent perspective. **Ensure your validation code also starts by loading `data.csv`.**
"""




def main():
    genai.configure(api_key=API_KEY)
    #model = genai.GenerativeModel('gemini-pro')
    model = genai.GenerativeModel('gemini-2.0-flash')


    # Ensure output folder exists
    os.makedirs(OUTPUT_FOLDER, exist_ok=True)

    # Load Specification Document
    with open(SPECIFICATION_FILE, 'r') as f:
        specification_doc = f.read()

    # Create data.csv
    data_for_csv = {
        'SubjectID': [101, 102, 103, 104, 105],
        'DateOfBirth': ['1985-07-20', '2002-11-05', '1960-04-12', '2010-09-28', '1978-01-15'],
        'StudyDate': ['2024-08-20', '2024-08-20', '2024-08-20', '2024-08-20', '2024-08-20'],
        'SmokingStatus': ['Never Smoker', 'Current Smoker', 'Former Smoker', 'Never Smoker', 'Current Smoker']
    }
    dm_df_original = pd.DataFrame(data_for_csv)
    dm_df_original.to_csv('data.csv', index=False) # Save to CSV


    dm_dataset_description = "DM dataset (from data.csv) columns: " + ", ".join(dm_df_original.columns)

    validator_agent = model.start_chat() # Initiate the chat without system prompt
    validator_agent.send_message(validator_system_prompt_iterative_v4) # Send the system prompt as a message

    programmer_agent = model.start_chat()  # Initiate the chat without system prompt
    programmer_agent.send_message(programmer_system_prompt_iterative_v4) # Send the system prompt as a message

    comparison_history = []
    column_specifications = parse_specification_document(specification_doc)

    passed_columns_report = []
    failed_columns_report = []

    # --- Log File Setup ---
    timestamp_start = time.strftime("%Y-%m-%d_%H-%M-%S")
    log_filename = os.path.join(OUTPUT_FOLDER, f"execution_log_{timestamp_start}.txt")
    original_stdout = sys.stdout  # Store original stdout
    sys.stdout = open(log_filename, 'w') # Redirect stdout to log file
    print(f"Execution log saving to: {log_filename}") # Print message to log file (and original stdout)
    # --- End Log File Setup ---


    try: # try-finally block to ensure stdout is reset
        for column_spec in column_specifications:
            column_name = column_spec['column_name']
            spec_text = format_specification_for_agent(column_spec)
            iteration_count = 0
            programmer_code = None
            validation_report_text = None
            validation_assessment = "Fail" # Default to Fail
            output_matched_in_iteration = False
            programmer_exec_status = "Fail" # Track Programmer code execution status
            validator_exec_status = "Fail"  # Track Validator code execution status
            output_comparison_status = "Fail" # Track DataFrame output comparison status
            code_similarity_score = 0.0 # Initialize code similarity score


            while validation_assessment != f"{ANSI_GREEN}Pass{ANSI_RESET}" and iteration_count < MAX_ITERATIONS_PER_COLUMN: # Use color-coded Pass for loop condition
                iteration_count += 1
                timestamp = time.strftime("%Y-%m-%d %H:%M:%S")

                print(f"\n{'='*50} {ANSI_CYAN}ITERATION {iteration_count} - Column: {column_name}{ANSI_RESET} {'='*50}")

                # Programmer Agent Turn
                print(f"\n{timestamp} - {ANSI_BLUE}{ANSI_BOLD}Programmer:{ANSI_RESET} Generating code (Iteration {iteration_count})") # Chat-style log + BOLD
                programmer_prompt_content = spec_text + "\n\nDataset Description: " + dm_dataset_description
                if validation_report_text:
                    programmer_prompt_content += "\n\n**Previous Validation Report:**\n" + validation_report_text
                programmer_response = programmer_agent.send_message(programmer_prompt_content)
                programmer_code = extract_code_from_markdown(programmer_response.text) # EXTRACT PROGRAMMER CODE HERE
                print(f"\n{ANSI_BLUE}Programmer Code Generated (Iteration {iteration_count}):{ANSI_RESET}\n`python\n{programmer_code}\n`\n")
                save_agent_code(OUTPUT_FOLDER, column_name, "programmer", programmer_code)


                # Validator Agent Turn
                print(f"\n{timestamp} - {ANSI_MAGENTA}{ANSI_BOLD}Validator:{ANSI_RESET} Validating code (Iteration {iteration_count})") # Chat-style log + BOLD
                validation_prompt_content = spec_text + "\n\nProgrammer Code:\n" + programmer_code + "\n\nDataset Description: " + dm_dataset_description
                if validation_report_text:
                    validation_prompt_content += "\n\n**Previous Validation Report (for context):**\n" + validation_report_text

                validator_response = validator_agent.send_message(validation_prompt_content)
                validation_report_with_code = validator_response.text
                print(f"\n{ANSI_MAGENTA}Validator Response (Code & Report) (Iteration {iteration_count}):{ANSI_RESET}\n", validation_report_with_code) # Validator Response already contains code block usually
                save_agent_code(OUTPUT_FOLDER, column_name, "validator", programmer_code)

                validator_code = extract_code_from_markdown(validator_response.text) # EXTRACT VALIDATOR CODE HERE
                validation_report_text = extract_validation_report_text_from_response(validation_report_with_code)

                # Code Similarity Check
                code_similarity_score = calculate_code_similarity(programmer_code, validator_code)
                print(f"\nCode Similarity (Programmer vs. Validator): {code_similarity_score:.4f}") # Log similarity score
                if code_similarity_score >= CODE_SIMILARITY_THRESHOLD:
                    validation_report_text += f"\n\n{ANSI_YELLOW}Warning: Programmer and Validator code similarity is high ({code_similarity_score:.4f} >= {CODE_SIMILARITY_THRESHOLD}). Consider revising Validator code for more independent validation.{ANSI_RESET}" # Add warning to report
                    print(f"\n{ANSI_YELLOW}WARNING: High Code Similarity - {column_name} - Iteration {iteration_count}{ANSI_RESET}") # Highlight in logs

                # Note: We are now intentionally NOT extracting assessment from Validator Report itself
                # validation_assessment = extract_validation_assessment_from_response(validation_report_with_code) # DO NOT USE THIS - assessment is determined by Python script

                # Execute and Compare
                programmer_output_df = pd.read_csv('data.csv') # Load CSV in each iteration for Programmer and Validator
                validator_output_df = pd.read_csv('data.csv')


                programmer_code_exec_success = True
                validator_code_exec_success = True
                programmer_error_message = None # Capture specific error messages
                validator_error_message = None


                try:
                    # Code display during execution removed
                    exec(programmer_code, globals(), locals())
                    print(f"\n{timestamp} - {ANSI_GREEN}Programmer Code executed successfully.{ANSI_RESET}")
                    programmer_exec_status = "Pass" # Programmer code execution success
                except Exception as e:
                    programmer_error_message = str(e) # Capture error message
                    print(f"\n{timestamp} - {ANSI_RED}Error executing Programmer Code:{ANSI_RESET} {programmer_error_message}")
                    programmer_exec_status = "Fail" # Programmer code execution failure
                    validation_assessment = f"{ANSI_RED}Fail (Code Error){ANSI_RESET}" # Color-coded status - Code Error
                    validation_report_text = f"Programmer Code Execution Error: {programmer_error_message}\n\nReview Python syntax and logic. Correct syntax errors and ensure the code runs without errors."
                    programmer_code_exec_success = False

                try:
                    # Code display during execution removed
                    exec(validator_code, globals(), locals())
                    print(f"\n{timestamp} - {ANSI_GREEN}Validator Code executed successfully.{ANSI_RESET}")
                    validator_exec_status = "Pass" # Validator code execution success
                except Exception as e:
                    validator_error_message = str(e) # Capture error message
                    print(f"\n{timestamp} - {ANSI_RED}Error executing Validator Code:{ANSI_RESET} {validator_error_message}")
                    validator_exec_status = "Fail" # Validator code execution failure
                    validation_assessment = f"{ANSI_RED}Fail (Validator Code Error){ANSI_RESET}" # Color-coded status - Validator Code Error - though this should ideally not happen if validator code is correct
                    validation_report_text += f"\n\nValidator Code Execution Error: {validator_error_message}\n\nReview your validation code for syntax or logical errors. Your validation code itself should be correct."
                    validator_code_exec_success = False

                if programmer_code_exec_success and validator_code_exec_success:
                    comparison_result = compare_dataframes(programmer_output_df, validator_output_df, column_name=column_name)
                    comparison_history.append({
                        "column": column_name,
                        "iteration": iteration_count,
                        "assessment": validation_assessment,
                        "result": comparison_result,
                        "programmer_code": programmer_code,
                        "validator_code": validator_code,
                        "report": validation_report_text,
                        "programmer_error": programmer_error_message, # Store error messages if any
                        "validator_error": validator_error_message,
                        "programmer_exec_status": programmer_exec_status, # Store execution statuses
                        "validator_exec_status": validator_exec_status,
                        "output_comparison_status": output_comparison_status, # Store output comparison status
                        "code_similarity_score": code_similarity_score # Store code similarity score
                    })


                    if "IDENTICAL" in comparison_result.upper(): # Output Matched! - Check ONLY comparison, not validator's assessment
                        validation_assessment = f"{ANSI_GREEN}Pass{ANSI_RESET}" # Overall Pass - color-coded
                        output_comparison_status = "Pass" # Output comparison success
                        validation_report_text = "Code produced identical output and passed validation. Proceeding to next column."
                        print(f"\n{timestamp} - {ANSI_GREEN}Validation PASSED for {column_name} after {iteration_count} iterations!{ANSI_RESET} - {ANSI_GREEN}Code Executed: OK, Output: Identical{ANSI_RESET}") # More detailed pass message
                        output_matched_in_iteration = True

                        # Save code on Pass
                        save_agent_code(OUTPUT_FOLDER, column_name, "programmer", programmer_code)
                        save_agent_code(OUTPUT_FOLDER, column_name, "validator", validator_code)

                        # Print side-by-side DataFrame output when match occurs
                        print(f"\n--- {ANSI_GREEN}Side-by-Side DataFrame Output (Programmer vs. Validator) - Column: {column_name}{ANSI_RESET} ---")
                        print("\n"+ ANSI_BLUE + ANSI_BOLD + "Programmer DataFrame:" + ANSI_RESET) # DataFrame Name display + BOLD
                        # Display all columns in Programmer DataFrame
                        print(programmer_output_df.to_string())
                        print("\n"+ ANSI_MAGENTA + ANSI_BOLD + "Validator DataFrame:" + ANSI_RESET) # DataFrame Name display + BOLD
                        # Display all columns in Validator DataFrame
                        print(validator_output_df.to_string())


                    else: # Outputs not identical - Validation Failed
                        validation_assessment = f"{ANSI_RED}Fail (Output Mismatch){ANSI_RESET}" # Color-coded status - Output Mismatch
                        output_comparison_status = "Fail" # Output comparison failure
                        validation_report_text = f"Output DataFrames are {ANSI_RED}NOT identical.{ANSI_RESET}\n\nComparison Result:\n{comparison_result}\n\n{validation_report_text}" # Add comparison to report, color-coded
                        print(f"\n{timestamp} - {ANSI_RED}Validation FAILED for {column_name} in iteration {iteration_count} - Output Mismatch.{ANSI_RESET} - {ANSI_GREEN}Code Executed: OK, Output: Different{ANSI_RESET} Feedback provided to Programmer.") # More detailed fail message with Exec OK, Output Different
                        print(f"\n--- {ANSI_RED}DataFrame Comparison - Column: {column_name}{ANSI_RESET} ---\n{comparison_result}") # Detailed comparison in RED
                else: # If code execution failed for Programmer or Validator (or both) - validation_assessment already set to "Fail (Code Error)" or "Fail (Validator Code Error)"
                    output_comparison_status = "N/A" # Output comparison not applicable if code execution failed


                # Print Iteration Summary Table
                iteration_summary_data = [
                    ["Column", column_name],
                    ["Iteration", iteration_count],
                    ["Overall Assessment", validation_assessment], # Combined assessment string
                    ["Prog. Exec", f"{ANSI_GREEN}OK{ANSI_RESET}" if programmer_exec_status == "Pass" else f"{ANSI_RED}Error{ANSI_RESET}"], # Color-coded exec status
                    ["Val. Exec", f"{ANSI_GREEN}OK{ANSI_RESET}" if validator_exec_status == "Pass" else f"{ANSI_RED}Error{ANSI_RESET}"],  # Color-coded exec status
                    ["Output Comp.", f"{ANSI_GREEN}Identical{ANSI_RESET}" if output_comparison_status == "Pass" else (f"{ANSI_RED}Different{ANSI_RESET}" if output_comparison_status == "Fail" else "N/A")], # Color-coded output comparison status
                    ["Code Similarity", f"{code_similarity_score:.4f}"], # Report code similarity score
                    ["Result", comparison_result if programmer_code_exec_success and validator_code_exec_success else f"{ANSI_RED}Code Execution Error{ANSI_RESET}"], # Color-coded error in table
                    ["Report", validation_report_text[:150] + "..." if validation_report_text else "No Report"]
                ]
                print(f"\n--- {ANSI_YELLOW}Iteration {iteration_count} Summary - Column: {column_name}{ANSI_RESET} ---")
                print(tabulate(iteration_summary_data, tablefmt="grid"))

                if output_matched_in_iteration:
                    break # Skip to next column if outputs matched in first iteration


            # After iteration loop (for summary report)
            if validation_assessment == f"{ANSI_GREEN}Pass{ANSI_RESET}": # Correctly check for color-coded Pass
                passed_columns_report.append(column_name)
            else:
                failed_columns_report.append({"column": column_name, "reason": validation_assessment})

    finally: # Ensure stdout is reset even if errors occur in loop
        sys.stdout.close() # Close log file
        sys.stdout = original_stdout # Reset stdout to original console
        print(f"\n{ANSI_GREEN}Execution completed and log saved to: {log_filename}{ANSI_RESET}") # Message to original stdout


    print(f"\n{'='*50} {ANSI_CYAN}FINAL CONSOLIDATED REPORT{ANSI_RESET} {'='*50}")

    print(f"\n--- {ANSI_CYAN}Final Comparison History (Detailed){ANSI_RESET} ---")
    for history_entry in comparison_history:
        print(f"\nColumn: {history_entry['column']}, Iteration: {history_entry['iteration']}")
        print(f"Overall Assessment: {history_entry['assessment']}") # Combined assessment string
        print(f"Programmer Exec Status: {ANSI_GREEN}OK{ANSI_RESET}" if history_entry['programmer_exec_status'] == "Pass" else f"{ANSI_RED}Error{ANSI_RESET}") # Color-coded exec status
        print(f"Validator Exec Status: {ANSI_GREEN}OK{ANSI_RESET}" if history_entry['validator_exec_status'] == "Pass" else f"{ANSI_RED}Error{ANSI_RESET}") # Color-coded exec status
        print(f"Output Comparison: {ANSI_GREEN}Identical{ANSI_RESET}" if history_entry['output_comparison_status'] == "Pass" else (f"{ANSI_RED}Different{ANSI_RESET}" if history_entry['output_comparison_status'] == "Fail" else "N/A")) # Color-coded output comparison
        print(f"Code Similarity: {history_entry['code_similarity_score']:.4f}") # Report code similarity
        print(f"Result: {history_entry['result']}")
        print(f"Report: {history_entry['report']}")
        if history_entry['programmer_error']: # Print error messages if they occurred
            print(f"\n{ANSI_RED}Programmer Error:{ANSI_RESET} {history_entry['programmer_error']}") # Color-coded error
        if history_entry['validator_error']:
            print(f"\n{ANSI_RED}Validator Error:{ANSI_RESET} {history_entry['validator_error']}") # Color-coded error
        print(f"\nProgrammer Code (Iteration {history_entry['iteration']}):\n`python\n{history_entry['programmer_code']}\n`")
        print(f"\nValidator Code (Iteration {history_entry['iteration']}):\n`python\n{history_entry['validator_code']}\n`")
        print("-" * 50)


    print(f"\n--- {ANSI_YELLOW}Summary Report{ANSI_RESET} ---")
    print("\nPassed Columns:")
    if passed_columns_report:
        for col in passed_columns_report:
            print(f"- {ANSI_GREEN}{col}{ANSI_RESET}") # Green color for passed columns
    else:
        print("None")

    print("\nFailed Columns:")
    if failed_columns_report:
        for item in failed_columns_report:
            print(f"- {ANSI_RED}{item['column']}{ANSI_RESET} - Reason: {item['reason']}") # Red color for failed columns and reason
    else:
        print("None")

    print(f"\n{'='*100}")
    print(f"\n{ANSI_CYAN}--- Workflow Completed ---{ANSI_RESET}")

# --- Helper Functions ---
def parse_specification_document(spec_doc_text):
    """Parses the specification document (plain text) and extracts column specifications."""
    specifications = []
    sections = spec_doc_text.strip().split("## Column:") # Split by column sections
    for section in sections[1:]: # Skip the first empty section
        lines = section.strip().splitlines()
        column_name = lines[0].strip() # First line after "## Column:" is column name
        description = ""
        logic_algo = ""
        input_data = ""
        current_section = None

        for line in lines[1:]: # Process rest of the lines in section
            line = line.strip()
            if line.startswith("Description:"):
                current_section = "description"
                description += line[len("Description:"):].strip() + " "
            elif line.startswith("Logic/Algorithm:"):
                current_section = "logic_algo"
                logic_algo += line[len("Logic/Algorithm:"):].strip() + "\n"
            elif line.startswith("Input Data:"):
                current_section = "input_data"
                input_data += line[len("Input Data:"):].strip() + "\n"
            elif current_section == "description":
                description += line + " "
            elif current_section == "logic_algo":
                logic_algo += line + "\n"
            elif current_section == "input_data":
                input_data += line + "\n"

        specifications.append({
            "column_name": column_name,
            "description": description.strip(),
            "logic_algo": logic_algo.strip(),
            "input_data": input_data.strip()
        })
    return specifications


def format_specification_for_agent(column_spec):
    """Formats a single column specification into text for agent prompts."""
    return f"""
## Column: {column_spec['column_name']}

Description: {column_spec['description']}

Logic/Algorithm:
{column_spec['logic_algo']}

Input Data:
{column_spec['input_data']}
"""


def extract_code_from_markdown(response_text):
    """Extracts Python code from a markdown response, removing `python and ` markers."""
    code_block_start = "```python"
    code_block_end = "```"
    start_index = response_text.find(code_block_start)
    if start_index != -1:
        start_index += len(code_block_start)
        end_index = response_text.find(code_block_end, start_index)
        if end_index != -1:
            return response_text[start_index:end_index].strip()
    return "" # Return empty string if code block not found


def extract_validation_report_text_from_response(response_text):
    """Extracts the validation report text, attempting to exclude code blocks. Improve parsing."""
    code_block_start = "```python"
    code_block_end = "```"
    report_text = ""
    start_index = 0
    while True:
        code_start = response_text.find(code_block_start, start_index)
        if code_start == -1: # No more code blocks
            report_text += response_text[start_index:] # Add remaining text
            break
        else:
            report_text += response_text[start_index:code_start] # Add text before code block
            code_end = response_text.find(code_block_end, code_start + len(code_block_start))
            if code_end == -1: # Unclosed code block (unlikely but handle)
                report_text += response_text[code_start:] # Add rest as report (problematic case)
                break
            else:
                start_index = code_end + len(code_block_end) # Move start index after code block
    return report_text.strip()


def extract_validation_assessment_from_response(response_text): # This function is now redundant and not used. Kept for backward compatibility if you have older code parts relying on it.
    return "Inconclusive" #  Assessment is now determined by the Python script based on DataFrame comparison.


def compare_dataframes(df1, df2, column_name):
    """Compares two DataFrames, including all columns, providing detailed mismatch info."""
    try:
        # Identify all columns present in either DataFrame
        all_columns = sorted(list(set(df1.columns) | set(df2.columns)))
        comparison_report = ""

        for col in all_columns:
            if col not in df1.columns or col not in df2.columns:
                comparison_report += f"Warning: Column '{col}' is present in only one DataFrame.\n"
                continue # Skip comparison for columns not in both

            if df1[col].equals(df2[col]):
                comparison_report += f"Columns '{col}' are identical.\n"
            else:
                diff_series = df1[col] != df2[col]
                diff_indices = diff_series[diff_series].index.tolist()
                num_diff = diff_series.sum()
                mismatch_details = f"Columns '{col}' are DIFFERENT. Number of mismatches: {num_diff}.\n"
                mismatch_details += "Detailed Mismatches (showing index, Programmer Value, Validator Value):\n"
                for index in diff_indices[:5]: # Show max 5 mismatches per column
                    prog_value = df1.loc[index, col]
                    val_value = df2.loc[index, col]
                    mismatch_details += f"- Index {index}: Programmer Value = {prog_value}, Validator Value = {val_value}\n"
                if num_diff > 5:
                    mismatch_details += "... (and more mismatches)\n"
                comparison_report += mismatch_details + "\n" # Add column's mismatch details

        if not comparison_report: # No content means DataFrames were identical in compared columns
            return f"DataFrames are identical across all compared columns. Comparison: Identical."
        else:
            return "DataFrames are DIFFERENT.\n\n" + comparison_report

    except Exception as e: # Catch potential comparison errors
        return f"Error during DataFrame comparison: {e}"



def calculate_code_similarity(code1, code2):
    """Calculates cosine similarity between two code strings after tokenization."""
    if not code1 or not code2: # Handle empty code strings
        return 0.0

    vectorizer = TfidfVectorizer(tokenizer=word_tokenize, stop_words='english') # Tokenizer and stop words
    tfidf_matrix = vectorizer.fit_transform([code1, code2]) # Create TF-IDF matrix
    similarity_matrix = cosine_similarity(tfidf_matrix) # Calculate cosine similarity
    return similarity_matrix[0][1] # Return similarity score between code1 and code2


def save_agent_code(output_folder, column_name, agent_type, code_string):
    """Saves agent code to a file in the specified output folder."""
    filename = os.path.join(output_folder, f"{column_name}_{agent_type}_code.py")
    with open(filename, "w") as f:
        f.write(code_string)
    print(f"Saved {agent_type} code for {column_name} to: {filename}")


def display_side_by_side(*dfs, spaces=4): # This function is now redundant as full dataframes are printed directly.
    """Displays Pandas DataFrames side by side in the console."""
    output = ""
    combined_data = zip(*dfs)
    for i, (df_prog, df_val) in enumerate(combined_data):
        output += df_prog.to_string()
        output += " " * spaces
        output += df_val.to_string()
        output += "\n"  # Newline after each row comparison
    return output



def extract_validation_assessment_from_response(response_text): # This function is now redundant and not used. Kept for backward compatibility if you have older code parts relying on it.
    return "Inconclusive" #  Assessment is now determined by the Python script based on DataFrame comparison.


if __name__ == "__main__":
    main()